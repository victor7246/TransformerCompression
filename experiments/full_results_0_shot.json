{
    "arc_challenge": {
        "acc,none": 0.24829351535836178,
        "acc_stderr,none": 0.012624912868089679,
        "acc_norm,none": 0.29948805460750855,
        "acc_norm_stderr,none": 0.013385021637313725,
        "alias": "arc_challenge"
    },
    "arc_easy": {
        "acc,none": 0.5593434343434344,
        "acc_stderr,none": 0.01018726463571219,
        "acc_norm,none": 0.4936868686868687,
        "acc_norm_stderr,none": 0.010258965668044465,
        "alias": "arc_easy"
    },
    "hellaswag": {
        "acc,none": 0.4303923521210914,
        "acc_stderr,none": 0.0049411916073175775,
        "acc_norm,none": 0.5576578370842462,
        "acc_norm_stderr,none": 0.004956494059865118,
        "alias": "hellaswag"
    },
    "mmlu_abstract_algebra": {
        "alias": "abstract_algebra",
        "acc,none": 0.3,
        "acc_stderr,none": 0.04605661864718382
    },
    "mmlu_business_ethics": {
        "alias": "business_ethics",
        "acc,none": 0.25,
        "acc_stderr,none": 0.04351941398892446
    },
    "mmlu_college_computer_science": {
        "alias": "college_computer_science",
        "acc,none": 0.23,
        "acc_stderr,none": 0.04229525846816507
    },
    "mmlu_college_mathematics": {
        "alias": "college_mathematics",
        "acc,none": 0.26,
        "acc_stderr,none": 0.0440844002276808
    },
    "mmlu_conceptual_physics": {
        "alias": "conceptual_physics",
        "acc,none": 0.2553191489361702,
        "acc_stderr,none": 0.028504856470514203
    },
    "mmlu_formal_logic": {
        "alias": "formal_logic",
        "acc,none": 0.1746031746031746,
        "acc_stderr,none": 0.03395490020856111
    },
    "mmlu_global_facts": {
        "alias": "global_facts",
        "acc,none": 0.23,
        "acc_stderr,none": 0.04229525846816507
    },
    "mmlu_machine_learning": {
        "alias": "machine_learning",
        "acc,none": 0.26785714285714285,
        "acc_stderr,none": 0.04203277291467762
    },
    "mmlu_miscellaneous": {
        "alias": "miscellaneous",
        "acc,none": 0.24904214559386972,
        "acc_stderr,none": 0.015464676163396064
    },
    "mmlu_philosophy": {
        "alias": "philosophy",
        "acc,none": 0.21543408360128619,
        "acc_stderr,none": 0.023350225475471442
    },
    "piqa": {
        "acc,none": 0.7143634385201306,
        "acc_stderr,none": 0.010539303948661828,
        "acc_norm,none": 0.7187159956474428,
        "acc_norm_stderr,none": 0.010490509832327499,
        "alias": "piqa"
    },
    "winogrande": {
        "acc,none": 0.5951065509076559,
        "acc_stderr,none": 0.013795927003124962,
        "alias": "winogrande"
    }
}