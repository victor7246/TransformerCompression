{
    "arc_challenge": {
        "acc,none": 0.22610921501706485,
        "acc_stderr,none": 0.012224202097063337,
        "acc_norm,none": 0.2627986348122867,
        "acc_norm_stderr,none": 0.012862523175351397,
        "alias": "arc_challenge"
    },
    "arc_easy": {
        "acc,none": 0.44823232323232326,
        "acc_stderr,none": 0.010204645126856943,
        "acc_norm,none": 0.4086700336700337,
        "acc_norm_stderr,none": 0.010087174498762844,
        "alias": "arc_easy"
    },
    "hellaswag": {
        "acc,none": 0.345947022505477,
        "acc_stderr,none": 0.004747038768172681,
        "acc_norm,none": 0.4162517426807409,
        "acc_norm_stderr,none": 0.004919289113027488,
        "alias": "hellaswag"
    },
    "mmlu_abstract_algebra": {
        "alias": "abstract_algebra",
        "acc,none": 0.22,
        "acc_stderr,none": 0.041633319989322654
    },
    "mmlu_business_ethics": {
        "alias": "business_ethics",
        "acc,none": 0.3,
        "acc_stderr,none": 0.04605661864718382
    },
    "mmlu_college_computer_science": {
        "alias": "college_computer_science",
        "acc,none": 0.25,
        "acc_stderr,none": 0.04351941398892446
    },
    "mmlu_college_mathematics": {
        "alias": "college_mathematics",
        "acc,none": 0.21,
        "acc_stderr,none": 0.040936018074033236
    },
    "mmlu_conceptual_physics": {
        "alias": "conceptual_physics",
        "acc,none": 0.26382978723404255,
        "acc_stderr,none": 0.028809989854102946
    },
    "mmlu_formal_logic": {
        "alias": "formal_logic",
        "acc,none": 0.29365079365079366,
        "acc_stderr,none": 0.04073524322147127
    },
    "mmlu_global_facts": {
        "alias": "global_facts",
        "acc,none": 0.18,
        "acc_stderr,none": 0.03861229196653691
    },
    "mmlu_machine_learning": {
        "alias": "machine_learning",
        "acc,none": 0.3125,
        "acc_stderr,none": 0.043994650575715215
    },
    "mmlu_miscellaneous": {
        "alias": "miscellaneous",
        "acc,none": 0.23627075351213284,
        "acc_stderr,none": 0.01519047371703751
    },
    "mmlu_philosophy": {
        "alias": "philosophy",
        "acc,none": 0.1864951768488746,
        "acc_stderr,none": 0.022122439772480733
    },
    "piqa": {
        "acc,none": 0.6539717083786725,
        "acc_stderr,none": 0.011098919626957283,
        "acc_norm,none": 0.6599564744287268,
        "acc_norm_stderr,none": 0.011052749414423359,
        "alias": "piqa"
    },
    "winogrande": {
        "acc,none": 0.5564325177584846,
        "acc_stderr,none": 0.013962694907620256,
        "alias": "winogrande"
    }
}